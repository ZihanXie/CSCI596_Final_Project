{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2K_brRjXpS_A",
        "outputId": "075dec5f-1c0c-495d-da43-ce592173fd7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.11.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.98)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.11.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.9.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.11.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.28.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.15.1+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.1.98)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.13.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: POT in /usr/local/lib/python3.9/dist-packages (0.9.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.9/dist-packages (from POT) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.9/dist-packages (from POT) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
            "/usr/lib/python3.9/runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install sentencepiece\n",
        "!pip install datasets\n",
        "!pip install -U sentence-transformers\n",
        "!pip install POT\n",
        "!pip install nltk\n",
        "!python -m nltk.downloader punkt\n",
        "!pip install rouge\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3m3HqLMqZU0"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel, T5ForConditionalGeneration, T5Tokenizer\n",
        "import torch\n",
        "import datasets\n",
        "import statistics\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import download\n",
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF1gkBYZjE56"
      },
      "source": [
        "### Load Similarity model to check accuracy\n",
        "Documentation:https://huggingface.co/tasks/sentence-similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDq3fwyOa2L8"
      },
      "outputs": [],
      "source": [
        "\n",
        "similarity_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "\"\"\"\n",
        "# predicted_answer: List of Predicted answers\n",
        "# true_answers: List of corresponding true answers\n",
        "\"\"\"\n",
        "def sentences_similarity(predicted_answers, true_answers):\n",
        "  similarity = []\n",
        "  for i in range(len(predicted_answers)):\n",
        "    sentences = [predicted_answers[i],true_answers[i]]\n",
        "    #Compute embedding for both lists\n",
        "    embedding_1= similarity_model.encode(sentences[0], convert_to_tensor=True)\n",
        "    embedding_2 = similarity_model.encode(sentences[1], convert_to_tensor=True)\n",
        "    score = util.pytorch_cos_sim(embedding_1, embedding_2)\n",
        "    score = score.item()\n",
        "    similarity.append(score)\n",
        "  average = statistics.mean(similarity)\n",
        "  return average\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkHTCiWcxa69"
      },
      "source": [
        "###Function for model evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O6BV_b1heq-"
      },
      "outputs": [],
      "source": [
        "def model_evalutate(model,dataset,count):\n",
        "  counter = 0\n",
        "  predict_arr = []\n",
        "  true_arr = []\n",
        "  for example in dataset:\n",
        "    context = example['context']\n",
        "    questions = example['question']\n",
        "    answers = example['answers']\n",
        "    # Check if there is at least one answer\n",
        "    if answers and len(answers['text'])>0:        \n",
        "        counter += 1\n",
        "        answer_text = answers['text'][0]\n",
        "        \n",
        "        question_pred = model(answer_text,context)\n",
        "        # Extract the answer text and start/end positions\n",
        "        predict_arr.append(question_pred)\n",
        "        # print(answer_text)\n",
        "        # print(context)\n",
        "\n",
        "        # print(questions)\n",
        "\n",
        "        # print(question_pred)\n",
        "        # print(\"---------------------------------------------------\")\n",
        "        true_arr.append(questions)\n",
        "        if counter >= count:\n",
        "          break\n",
        "  return (true_arr,predict_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ak1fdmujP2J"
      },
      "source": [
        "###Load Our tuned t5-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eJjagHFpu_g",
        "outputId": "670897ad-21be-4d97-8d82-a42565cffe73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What does GDP stand for?\n"
          ]
        }
      ],
      "source": [
        "our_en_tokenizer = T5Tokenizer.from_pretrained(\"ZihanXie/QuestionGeneration\")\n",
        "our_en_model = T5ForConditionalGeneration.from_pretrained(\"ZihanXie/QuestionGeneration\")\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "our_en_model.to(device)\n",
        "\n",
        "\n",
        "def our_en_run_model(answer, context, **generator_args):\n",
        "  generator_args = {\n",
        "  \"max_length\": 256,\n",
        "  \"num_beams\": 4,\n",
        "  \"length_penalty\": 1.5,\n",
        "  \"no_repeat_ngram_size\": 3,\n",
        "  \"early_stopping\": True,\n",
        "  }\n",
        "  input_string = 'answer:' + answer+\" context:\" + context\n",
        "  input_string = '%s </s>' % (input_string)\n",
        "  input_ids = our_en_tokenizer.encode(input_string, return_tensors=\"pt\")\n",
        "  res = our_en_model.generate(input_ids.to(device), **generator_args)\n",
        "  output = our_en_tokenizer.batch_decode(res, skip_special_tokens=True)\n",
        "  return output[0]\n",
        "\n",
        "output = our_en_run_model('Gross domestic product ','GDP means gross domestic product.')\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY64vqxSjZ8R"
      },
      "source": [
        "### Load Base finetuned t5-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "v27SXsL6bbVG",
        "outputId": "80fea23a-67fd-4ae0-a568-4d6a8ac24ba6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/auto/modeling_auto.py:1322: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n",
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Where is Normandy located?'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "t5_tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
        "t5_model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
        "\n",
        "def t5_run_model(answer, context, max_length=520):\n",
        "  \n",
        "  input_text = \"answer: %s  context: %s </s>\" % (answer, context)\n",
        "  features = t5_tokenizer([input_text], return_tensors='pt')\n",
        "\n",
        "  output = t5_model.generate(input_ids=features['input_ids'], \n",
        "               attention_mask=features['attention_mask'],\n",
        "               max_length=max_length)\n",
        "\n",
        "  return t5_tokenizer.decode(output[0])[16:-4]\n",
        "\n",
        "context =\"\"\"The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\"\"\"\n",
        "\n",
        "answer =\"France\"\n",
        "\n",
        "t5_run_model(answer, context)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMeNBUyjxYRz"
      },
      "source": [
        "### Load Base mt5 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v0KMfm3xcJ9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "mt5_tokenizer = AutoTokenizer.from_pretrained(\"Narrativa/mT5-base-finetuned-tydiQA-question-generation\")\n",
        "\n",
        "mt5_model = AutoModelForSeq2SeqLM.from_pretrained(\"Narrativa/mT5-base-finetuned-tydiQA-question-generation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0ePzT0lGxgG7",
        "outputId": "23b295b5-33b2-4610-e322-a3dbfb87e043"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What was Sofía Martínez-Avila responsible for PR?'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def mt5_run_model(answer, context, max_length=520):\n",
        "  input_text = \"answer: %s  context: %s </s>\" % (answer, context)\n",
        "  features = mt5_tokenizer([input_text], return_tensors='pt')\n",
        "\n",
        "  output = mt5_model.generate(input_ids=features['input_ids'], \n",
        "               attention_mask=features['attention_mask'],\n",
        "               max_length=max_length)\n",
        "\n",
        "  return mt5_tokenizer.decode(output[0])[6:-4]\n",
        "\n",
        "context =\"\"\" Sofía has a degree in Communications and public relations agency experience where she was in charge of monitoring and managing PR strategy including relations with the media and journalists.\"\"\"\n",
        "\n",
        "answer =\"monitoring and managing PR strategy including relations with the media and journalists\"\n",
        "\n",
        "mt5_run_model(answer, context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y1fbMXBV9kc",
        "outputId": "d69f79a0-9163-49ba-9b14-e8fdec654a04"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " What is the name of the item that is on the counter?\n"
          ]
        }
      ],
      "source": [
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\",pad_token=\"<pad>\")\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"danyaljj/gpt2_question_generation_given_paragraph_answer\",  pad_token_id=gpt2_tokenizer.eos_token_id)\n",
        "\n",
        "def gpt2_run_model(answer, context, max_length=512):\n",
        "  # input_text = \"answer: %s  context: %s </s>\" % (answer, context)\n",
        "  input_text = context+\" A: \"+ answer+\" Q:\"\n",
        "  features = gpt2_tokenizer([input_text],  return_tensors='pt')\n",
        "  output = gpt2_model.generate(input_ids=features['input_ids'], attention_mask=features['attention_mask'],max_length=max_length)\n",
        "  # input_ids = tokenizer.encode(input_str, return_tensors=\"pt\")\n",
        "  # print(input_ids)\n",
        "  # print(input_str)\n",
        "  # outputs = model.generate(input_ids,max_length=max_length)\n",
        "  return (gpt2_tokenizer.decode(output[0], skip_special_tokens=True)[len(input_text):])\n",
        "print(gpt2_run_model(\"apples\", \"There are two apples on the counter.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hStjzZHGjpGs"
      },
      "source": [
        "### Load testing data\n",
        "squad_v2: https://huggingface.co/datasets/GEM/squad_v2/viewer/gem_data_split/test  \n",
        "cmrc2018:https://github.com/ymcui/cmrc2018"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmIfoK8FWZ3-",
        "outputId": "0efc6e97-5110-4d53-dada-a0373b8ef8b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:No config specified, defaulting to: squad_v2/gem_data_split\n",
            "WARNING:datasets.builder:Found cached dataset squad_v2 (/root/.cache/huggingface/datasets/GEM___squad_v2/gem_data_split/1.0.0/13af4f8baf72eab5de773cce096ca00be63c10441de58c7b629692125ab8a3ac)\n",
            "WARNING:datasets.builder:Found cached dataset cmrc2018 (/root/.cache/huggingface/datasets/cmrc2018/default/0.1.0/3cbb788a586e4597f67937944006349cd758baef9409fb90a6ddb85c1c84690c)\n",
            "WARNING:datasets.builder:Found cached dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
            "WARNING:datasets.builder:Found cached dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n"
          ]
        }
      ],
      "source": [
        "squad_v2 = datasets.load_dataset(\"GEM/squad_v2\", split=\"test\")\n",
        "cmrc2018 = datasets.load_dataset('cmrc2018', split=\"test\")\n",
        "\n",
        "dataset = datasets.load_dataset(\"squad\", split=\"validation\")\n",
        "\"badokorach/NewQA\"\n",
        "squadv2 = datasets.load_dataset(\"squad_v2\", split=\"validation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrQ3D2hypsL8"
      },
      "source": [
        "### Validation on our model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59BnlcWmj-lB"
      },
      "source": [
        "###EN our"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrUJTLbln-yq",
        "outputId": "3b6bdb45-3a93-4922-c017-971e8f52bd76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "300 300\n",
            "The average of our EN model: 0.8001428450768192\n"
          ]
        }
      ],
      "source": [
        "our_en_true, our_en_pred = model_evalutate(our_en_run_model,squad_v2,300 )\n",
        "print(len(our_en_true),(len(our_en_pred)))\n",
        "average = sentences_similarity(our_en_true, our_en_pred)\n",
        "print(\"The average of our EN model:\", average)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIpSKtedkLVs"
      },
      "source": [
        "### EN T5\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpA_1_JZR6SZ",
        "outputId": "7b713826-a20e-4e57-cd6b-45983adaaa8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The average of t5 model: 0.8193745869894823\n"
          ]
        }
      ],
      "source": [
        "t5_true, t5_pred = model_evalutate(t5_run_model,squad_v2,300 )\n",
        "average = sentences_similarity(t5_true, t5_pred)\n",
        "print(\"The average of t5 model:\", average)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVhYS8IlxsL8"
      },
      "source": [
        "### EN mt5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ylc38zJoxuxi",
        "outputId": "35034df2-ba04-4b4e-81d1-05d514532fdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The average of mt5 model on EN: 0.5601809611637145\n"
          ]
        }
      ],
      "source": [
        "mt5_en_true, mt5_en_pred = model_evalutate(mt5_run_model,squad_v2,300 )\n",
        "average = sentences_similarity(mt5_en_true, mt5_en_pred)\n",
        "print(\"The average of mt5 model on EN:\", average)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri_CpXzEqws4"
      },
      "source": [
        "### EN GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-jZpnG-cwvt",
        "outputId": "849f9418-f191-4fa8-8eee-89b28818d499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The average of gpt2 model on EN: 0.7408919122070075\n"
          ]
        }
      ],
      "source": [
        "gpt2_en_true, gpt2_en_pred = model_evalutate(gpt2_run_model,squad_v2,300 )\n",
        "average = sentences_similarity(gpt2_en_true, gpt2_en_pred)\n",
        "print(\"The average of gpt2 model on EN:\", average)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSmFMeOvTpOC"
      },
      "source": [
        "### Word Mover Distance Model\n",
        "A lower WMD value indicates higher similarity between the texts, while a higher WMD value indicates lower similarity or higher dissimilarity between the texts.\n",
        "\n",
        "Documentation:https://radimrehurek.com/gensim/auto_examples/tutorials/run_wmd.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si5lIQ3XB85p",
        "outputId": "84288a25-bfc9-434e-c496-e44f94d92575"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Import and download stopwords from NLTK.\n",
        "word2vec_model = api.load('word2vec-google-news-300') \n",
        "download('stopwords') \n",
        "stop_words = stopwords.words('english')\n",
        "def preprocess(sentence):\n",
        "    return [w for w in sentence.lower().split() if w not in stop_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoZlS7VNLC8F"
      },
      "outputs": [],
      "source": [
        "def WMD(sentence1, sentence2):\n",
        "\n",
        "  processed_s1 = preprocess(sentence1)\n",
        "  processed_s2 = preprocess(sentence2)\n",
        "  distance = word2vec_model.wmdistance(processed_s1, processed_s2)\n",
        "  # print('distance = %.4f' % distance)\n",
        "  return distance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tSCjn-cMjW3"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def model_WMD(true_answers, predicted_answers):\n",
        "  distance = []\n",
        "  for i in range(len(predicted_answers)):\n",
        "    #Compute embedding for both lists\n",
        "    \n",
        "    wmd = WMD(true_answers[i], predicted_answers[i])\n",
        "    if math.isinf(wmd):\n",
        "      continue\n",
        "    else:\n",
        "      distance.append(wmd)\n",
        "  average = statistics.mean(distance)\n",
        "  return average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hzVgm-fRShA",
        "outputId": "983161e6-da3d-4ad6-d0ff-0d3fd5c68f05"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our model WMD:  0.5985124385375024\n"
          ]
        }
      ],
      "source": [
        "our_wmd = model_WMD(our_en_true, our_en_pred)\n",
        "print(\"Our model WMD: \",our_wmd)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nE3STBQSz8w",
        "outputId": "d7f51d45-74e1-42f2-c319-d347489d415e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t5 model WMD:  0.5983119764177492\n"
          ]
        }
      ],
      "source": [
        "t5_wmd = model_WMD(t5_true, t5_pred)\n",
        "print(\"t5 model WMD: \",t5_wmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE4Rm8-LS0Ft",
        "outputId": "f7f028de-4b5e-4c9e-d322-bef5ac98dc73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mt-5 model WMD:  1.0819367661411814\n"
          ]
        }
      ],
      "source": [
        "mt5_wmd = model_WMD(mt5_en_true, mt5_en_pred)\n",
        "print(\"mt-5 model WMD: \",mt5_wmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX92C3AvsmOo",
        "outputId": "f4a24468-4a53-4127-815e-d6988c6bea7b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
            "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gpt2 model WMD:  0.7264586745445931\n"
          ]
        }
      ],
      "source": [
        "gpt2_wmd = model_WMD(gpt2_en_true, gpt2_en_pred)\n",
        "print(\"gpt2 model WMD: \",gpt2_wmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBJ9yVWfAAnM"
      },
      "outputs": [],
      "source": [
        "def get_BLEU_score_en(true_answers, predicted_answers):\n",
        "  scores = []\n",
        "  for i in range(len(predicted_answers)):\n",
        "    #Compute embedding for both lists\n",
        "\n",
        "    tokens1 = word_tokenize(true_answers[i])\n",
        "    tokens2 = word_tokenize(predicted_answers[i])\n",
        "\n",
        "    bleu_score = sentence_bleu([tokens1], tokens2)\n",
        "    scores.append(bleu_score)\n",
        "  average = statistics.mean(scores)\n",
        "  return average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSao7NPTAYgT",
        "outputId": "f32f00f7-bae0-4980-f1b8-12c9d1bdea76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "our en BLEU: 0.31606019515298467\n",
            "t5 en BLEU: 0.32948509830950196\n",
            "mt5 en BLEU: 0.03622468974179872\n",
            "gpt2 en BLEU: 0.24380164090843812\n"
          ]
        }
      ],
      "source": [
        "\n",
        "our_en_bleu = get_BLEU_score_en(our_en_true, our_en_pred)\n",
        "t5_en_bleu = get_BLEU_score_en(t5_true, t5_pred)\n",
        "mt5_en_bleu = get_BLEU_score_en(mt5_en_true, mt5_en_pred)\n",
        "gpt2_en_bleu = get_BLEU_score_en(gpt2_en_true, gpt2_en_pred)\n",
        "\n",
        "print(\"our en BLEU:\",our_en_bleu)\n",
        "print(\"t5 en BLEU:\",t5_en_bleu)\n",
        "print(\"mt5 en BLEU:\",mt5_en_bleu)\n",
        "print(\"gpt2 en BLEU:\",gpt2_en_bleu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Abe3JRd0HSZY",
        "outputId": "ab920f7d-24c9-453a-cae8-c7b4d703296a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rouge_our\n",
            "ROUGE-1: rouge-1\n",
            "{'r': 0.5957828952809604, 'p': 0.5442610569398497, 'f': 0.5591902924410386}\n",
            "ROUGE-2: rouge-2\n",
            "{'r': 0.4297418550947963, 'p': 0.3935395678939796, 'f': 0.4027162346350833}\n",
            "ROUGE-3: rouge-l\n",
            "{'r': 0.5748032624410335, 'p': 0.5255058300116353, 'f': 0.5398041494509297}\n"
          ]
        }
      ],
      "source": [
        "from rouge import Rouge\n",
        "rouge = Rouge()\n",
        "# Compute ROUGE-1, ROUGE-2, and ROUGE-L scores for sentences1 and sentences2 separately\n",
        "scores1 = rouge.get_scores(our_en_true, our_en_pred, avg=True)\n",
        "# print(scores1)\n",
        "# Print the scores\n",
        "print(\"rouge_our\")\n",
        "for i, s in enumerate(scores1):\n",
        "    print(f\"ROUGE-{i+1}: {s}\")\n",
        "    print(scores1[s])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDnmwShP7lbo",
        "outputId": "eeff633e-a5f4-49c5-ca0b-a56ac6b75a75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rouge_t5\n",
            "ROUGE-1: rouge-1\n",
            "{'r': 0.5939308800218244, 'p': 0.5617119601101805, 'f': 0.5686279245240401}\n",
            "ROUGE-2: rouge-2\n",
            "{'r': 0.4231218843674086, 'p': 0.40781975633299145, 'f': 0.4086386558962401}\n",
            "ROUGE-3: rouge-l\n",
            "{'r': 0.5731601619055181, 'p': 0.5439793957046282, 'f': 0.5499712815412329}\n"
          ]
        }
      ],
      "source": [
        "rouge_t5 = rouge.get_scores(t5_true, t5_pred, avg=True)\n",
        "# print(scores1)\n",
        "# Print the scores\\\n",
        "print(\"rouge_t5\")\n",
        "\n",
        "for i, s in enumerate(rouge_t5):\n",
        "    print(f\"ROUGE-{i+1}: {s}\")\n",
        "    print(rouge_t5[s])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhPPKlmr74Jo",
        "outputId": "a5db1472-4fe6-485e-bf5b-7cea896a05cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rouge_mt5\n",
            "ROUGE-1: rouge-1\n",
            "{'r': 0.31205278980279, 'p': 0.2371024282448432, 'f': 0.2622402134278826}\n",
            "ROUGE-2: rouge-2\n",
            "{'r': 0.10727248677248674, 'p': 0.08820368656398068, 'f': 0.09405312699455814}\n",
            "ROUGE-3: rouge-l\n",
            "{'r': 0.2956730399230401, 'p': 0.22651758750365572, 'f': 0.24969364226640156}\n"
          ]
        }
      ],
      "source": [
        "rouge_mt5 = rouge.get_scores(mt5_en_true, mt5_en_pred, avg=True)\n",
        "# print(scores1)\n",
        "# Print the scores\n",
        "print(\"rouge_mt5\")\n",
        "\n",
        "for i, s in enumerate(rouge_mt5):\n",
        "    print(f\"ROUGE-{i+1}: {s}\")\n",
        "    print(rouge_mt5[s])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg6faYpk7_9M",
        "outputId": "7b3033d1-1c46-4da8-8481-a0a19f826972"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rouge_gpt2\n",
            "ROUGE-1: rouge-1\n",
            "{'r': 0.5236675952871608, 'p': 0.47156020546074745, 'f': 0.4848521826967172}\n",
            "ROUGE-2: rouge-2\n",
            "{'r': 0.3483190444459798, 'p': 0.31177375772375787, 'f': 0.3211281645698845}\n",
            "ROUGE-3: rouge-l\n",
            "{'r': 0.5046402772892544, 'p': 0.45452173671987905, 'f': 0.4674318254930588}\n"
          ]
        }
      ],
      "source": [
        "rouge_gpt2 = rouge.get_scores(gpt2_en_true,gpt2_en_pred,avg=True)\n",
        "# print(scores1)\n",
        "# Print the scores\n",
        "print(\"rouge_gpt2\")\n",
        "\n",
        "for i, s in enumerate(rouge_gpt2):\n",
        "    print(f\"ROUGE-{i+1}: {s}\")\n",
        "    print(rouge_gpt2[s])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
